{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 簡単な例"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 1 item                                                               \u001b[0m\n\ntest_one.py \u001b[32m.\u001b[0m\u001b[32m                                                            [100%]\u001b[0m\n\n\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_one.py"
   ]
  },
  {
   "source": [
    "`.`は一つのテストが実行され無事終了の意味  \n",
    "もっと詳しい情報欲しい場合は`-v`あるいは`-verbose`を追加"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 -- /Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8\ncachedir: .pytest_cache\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 1 item                                                               \u001b[0m\n\ntest_one.py::test_passing \u001b[32mPASSED\u001b[0m\u001b[32m                                         [100%]\u001b[0m\n\n\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v test_one.py"
   ]
  },
  {
   "source": [
    "失敗する場合"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_two.py"
   ]
  },
  {
   "source": [
    "`Use -v to get the full diff`が出てくるのでそうしてもらう"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 -- /Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_two.py::test_failing \u001b[31mFAILED\u001b[0m\u001b[31m                                         [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Full diff:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?  ^     ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + (1, 2, 3)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?  ^     ^\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v test_two.py"
   ]
  },
  {
   "source": [
    "今回は違うところに`^`が表記された"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 実行"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n\npositional arguments:\n  file_or_dir\n\ngeneral:\n  -k EXPRESSION         only run tests which match the given substring\n                        expression. An expression is a python evaluatable\n                        expression where all names are substring-matched against\n                        test names and their parent classes. Example: -k\n                        'test_method or test_other' matches all test functions\n                        and classes whose name contains 'test_method' or\n                        'test_other', while -k 'not test_method' matches those\n                        that don't contain 'test_method' in their names. -k 'not\n                        test_method and not test_other' will eliminate the\n                        matches. Additionally keywords are matched to classes\n                        and functions containing extra names in their\n                        'extra_keyword_matches' set, as well as functions which\n                        have names assigned directly to them. The matching is\n                        case-insensitive.\n  -m MARKEXPR           only run tests matching given mark expression.\n                        For example: -m 'mark1 and not mark2'.\n  --markers             show markers (builtin, plugin and per-project ones).\n  -x, --exitfirst       exit instantly on first error or failed test.\n  --fixtures, --funcargs\n                        show available fixtures, sorted by plugin appearance\n                        (fixtures with leading '_' are only shown with '-v')\n  --fixtures-per-test   show fixtures per test\n  --pdb                 start the interactive Python debugger on errors or\n                        KeyboardInterrupt.\n  --pdbcls=modulename:classname\n                        start a custom interactive Python debugger on errors.\n                        For example:\n                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n  --trace               Immediately break when running each test.\n  --capture=method      per-test capturing method: one of fd|sys|no|tee-sys.\n  -s                    shortcut for --capture=no.\n  --runxfail            report the results of xfail tests as if they were not\n                        marked\n  --lf, --last-failed   rerun only the tests that failed at the last run (or all\n                        if none failed)\n  --ff, --failed-first  run all tests, but run the last failures first.\n                        This may re-order tests and thus lead to repeated\n                        fixture setup/teardown.\n  --nf, --new-first     run tests from new files first, then the rest of the\n                        tests sorted by file mtime\n  --cache-show=[CACHESHOW]\n                        show cache contents, don't perform collection or tests.\n                        Optional argument: glob (default: '*').\n  --cache-clear         remove all cache contents at start of test run.\n  --lfnf={all,none}, --last-failed-no-failures={all,none}\n                        which tests to run with no previously (known) failures.\n  --sw, --stepwise      exit on test failure and continue from last failing test\n                        next time\n  --sw-skip, --stepwise-skip\n                        ignore the first failing test but stop on the next\n                        failing test\n\nreporting:\n  --durations=N         show N slowest setup/test durations (N=0 for all).\n  --durations-min=N     Minimal duration in seconds for inclusion in slowest\n                        list. Default 0.005\n  -v, --verbose         increase verbosity.\n  --no-header           disable header\n  --no-summary          disable summary\n  -q, --quiet           decrease verbosity.\n  --verbosity=VERBOSE   set verbosity. Default is 0.\n  -r chars              show extra test summary info as specified by chars:\n                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,\n                        (p)assed, (P)assed with output, (a)ll except passed\n                        (p/P), or (A)ll. (w)arnings are enabled by default (see\n                        --disable-warnings), 'N' can be used to reset the list.\n                        (default: 'fE').\n  --disable-warnings, --disable-pytest-warnings\n                        disable warnings summary\n  -l, --showlocals      show locals in tracebacks (disabled by default).\n  --tb=style            traceback print mode (auto/long/short/line/native/no).\n  --show-capture={no,stdout,stderr,log,all}\n                        Controls how captured stdout/stderr/log is shown on\n                        failed tests. Default is 'all'.\n  --full-trace          don't cut any tracebacks (default is to cut).\n  --color=color         color terminal output (yes/no/auto).\n  --code-highlight={yes,no}\n                        Whether code should be highlighted (only if --color is\n                        also enabled)\n  --pastebin=mode       send failed|all info to bpaste.net pastebin service.\n  --junit-xml=path      create junit-xml style report file at given path.\n  --junit-prefix=str    prepend prefix to classnames in junit-xml output\n\npytest-warnings:\n  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n                        set which warnings to report, see -W option of python\n                        itself.\n  --maxfail=num         exit after first num failures or errors.\n  --strict-config       any warnings encountered while parsing the `pytest`\n                        section of the configuration file raise errors.\n  --strict-markers      markers not registered in the `markers` section of the\n                        configuration file raise errors.\n  --strict              (deprecated) alias to --strict-markers.\n  -c file               load configuration from `file` instead of trying to\n                        locate one of the implicit configuration files.\n  --continue-on-collection-errors\n                        Force test execution even if collection errors occur.\n  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n                        'root_dir', './root_dir', 'root_dir/another_dir/';\n                        absolute path: '/home/user/root_dir'; path with\n                        variables: '$HOME/root_dir'.\n\ncollection:\n  --collect-only, --co  only collect tests, don't execute them.\n  --pyargs              try to interpret all arguments as python packages.\n  --ignore=path         ignore path during collection (multi-allowed).\n  --ignore-glob=path    ignore path pattern during collection (multi-allowed).\n  --deselect=nodeid_prefix\n                        deselect item (via node id prefix) during collection\n                        (multi-allowed).\n  --confcutdir=dir      only load conftest.py's relative to specified dir.\n  --noconftest          Don't load any conftest.py files.\n  --keep-duplicates     Keep duplicate tests.\n  --collect-in-virtualenv\n                        Don't ignore tests in a local virtualenv directory\n  --import-mode={prepend,append,importlib}\n                        prepend/append to sys.path when importing test modules\n                        and conftest files, default is to prepend.\n  --doctest-modules     run doctests in all .py modules\n  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n                        choose another output format for diffs on doctest\n                        failure\n  --doctest-glob=pat    doctests file matching pattern, default: test*.txt\n  --doctest-ignore-import-errors\n                        ignore doctest ImportErrors\n  --doctest-continue-on-failure\n                        for a given doctest, continue to run after the first\n                        failure\n\ntest session debugging and configuration:\n  --basetemp=dir        base temporary directory for this test run.(warning:\n                        this directory is removed if it exists)\n  -V, --version         display pytest version and information about\n                        plugins.When given twice, also display information about\n                        plugins.\n  -h, --help            show help message and configuration info\n  -p name               early-load given plugin module name or entry point\n                        (multi-allowed).\n                        To avoid loading of plugins, use the `no:` prefix, e.g.\n                        `no:doctest`.\n  --trace-config        trace considerations of conftest.py files.\n  --debug               store internal tracing debug information in\n                        'pytestdebug.log'.\n  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n                        override ini option with \"option=value\" style, e.g. `-o\n                        xfail_strict=True -o cache_dir=cache`.\n  --assert=MODE         Control assertion debugging tools.\n                        'plain' performs no assertion debugging.\n                        'rewrite' (the default) rewrites assert statements in\n                        test modules on import to provide assert expression\n                        information.\n  --setup-only          only setup fixtures, do not execute tests.\n  --setup-show          show setup of fixtures while executing tests.\n  --setup-plan          show what fixtures and tests would be executed but don't\n                        execute anything.\n\nlogging:\n  --log-level=LEVEL     level of messages to catch/display.\n                        Not set by default, so it depends on the root/parent log\n                        handler's effective level, where it is \"WARNING\" by\n                        default.\n  --log-format=LOG_FORMAT\n                        log format as used by the logging module.\n  --log-date-format=LOG_DATE_FORMAT\n                        log date format as used by the logging module.\n  --log-cli-level=LOG_CLI_LEVEL\n                        cli logging level.\n  --log-cli-format=LOG_CLI_FORMAT\n                        log format as used by the logging module.\n  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n                        log date format as used by the logging module.\n  --log-file=LOG_FILE   path to a file when logging will be written to.\n  --log-file-level=LOG_FILE_LEVEL\n                        log file logging level.\n  --log-file-format=LOG_FILE_FORMAT\n                        log format as used by the logging module.\n  --log-file-date-format=LOG_FILE_DATE_FORMAT\n                        log date format as used by the logging module.\n  --log-auto-indent=LOG_AUTO_INDENT\n                        Auto-indent multiline messages passed to the logging\n                        module. Accepts true|on, false|off or an integer.\n\n[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:\n\n  markers (linelist):   markers for test functions\n  empty_parameter_set_mark (string):\n                        default marker for empty parametersets\n  norecursedirs (args): directory patterns to avoid for recursion\n  testpaths (args):     directories to search for tests when no files or\n                        directories are given in the command line.\n  filterwarnings (linelist):\n                        Each line specifies a pattern for\n                        warnings.filterwarnings. Processed after\n                        -W/--pythonwarnings.\n  usefixtures (args):   list of default fixtures to be used with this project\n  python_files (args):  glob-style file patterns for Python test module\n                        discovery\n  python_classes (args):\n                        prefixes or glob names for Python test class discovery\n  python_functions (args):\n                        prefixes or glob names for Python test function and\n                        method discovery\n  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n                        disable string escape non-ascii characters, might cause\n                        unwanted side effects(use at your own risk)\n  console_output_style (string):\n                        console output: \"classic\", or with additional progress\n                        information (\"progress\" (percentage) | \"count\").\n  xfail_strict (bool):  default for the strict parameter of xfail markers when\n                        not given explicitly (default: False)\n  enable_assertion_pass_hook (bool):\n                        Enables the pytest_assertion_pass hook.Make sure to\n                        delete any previously generated pyc cache files.\n  junit_suite_name (string):\n                        Test suite name for JUnit report\n  junit_logging (string):\n                        Write captured log messages to JUnit report: one of\n                        no|log|system-out|system-err|out-err|all\n  junit_log_passing_tests (bool):\n                        Capture log information for passing tests to JUnit\n                        report:\n  junit_duration_report (string):\n                        Duration time to report: one of total|call\n  junit_family (string):\n                        Emit XML for schema: one of legacy|xunit1|xunit2\n  doctest_optionflags (args):\n                        option flags for doctests\n  doctest_encoding (string):\n                        encoding used for doctest files\n  cache_dir (string):   cache directory path.\n  log_level (string):   default value for --log-level\n  log_format (string):  default value for --log-format\n  log_date_format (string):\n                        default value for --log-date-format\n  log_cli (bool):       enable log display during test run (also known as \"live\n                        logging\").\n  log_cli_level (string):\n                        default value for --log-cli-level\n  log_cli_format (string):\n                        default value for --log-cli-format\n  log_cli_date_format (string):\n                        default value for --log-cli-date-format\n  log_file (string):    default value for --log-file\n  log_file_level (string):\n                        default value for --log-file-level\n  log_file_format (string):\n                        default value for --log-file-format\n  log_file_date_format (string):\n                        default value for --log-file-date-format\n  log_auto_indent (string):\n                        default value for --log-auto-indent\n  faulthandler_timeout (string):\n                        Dump the traceback of all threads if a test takes more\n                        than TIMEOUT seconds to finish.\n  addopts (args):       extra command line options\n  minversion (string):  minimally required pytest version\n  required_plugins (args):\n                        plugins that must be present for pytest to run\n\nenvironment variables:\n  PYTEST_ADDOPTS           extra command line options\n  PYTEST_PLUGINS           comma-separated plugins to load during startup\n  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading\n  PYTEST_DEBUG             set to enable debug tracing of pytest's internals\n\n\nto see available markers type: pytest --markers\nto see available fixtures type: pytest --fixtures\n(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
     ]
    }
   ],
   "source": [
    "!pytest --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "test_four.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [ 33%]\u001b[0m\n",
      "test_one.py \u001b[32m.\u001b[0m\u001b[32m                                                            [ 50%]\u001b[0m\n",
      "test_three.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [ 83%]\u001b[0m\n",
      "test_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m5 passed\u001b[0m\u001b[31m in 0.25s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "source": [
    "パラメータがない時、再帰的に現在のフォルダからテストケースを見つけ実行する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 4 items                                                              \u001b[0m\n\ntest_three.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [ 50%]\u001b[0m\ntest_four.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [100%]\u001b[0m\n\n\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_three.py test_four.py"
   ]
  },
  {
   "source": [
    "このように、実行したいファイルを指定できる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 命名規則\n",
    "\n",
    "+ ファイル: `test_<something>.py` or `<something>_test.py`\n",
    "+ 関数、メソッド: `test_<something>`\n",
    "+ クラス: `Test<something>`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## テスト結果\n",
    "\n",
    "* PASSED(.): 成功\n",
    "* FAILED(F): 失敗\n",
    "* SKIPPED(s): スキップされた\n",
    "* xfail(x): 予想通り失敗した\n",
    "* XPASS(X):　予想外成功した(unexpectedly passing)\n",
    "* ERROR(E):　例外発生した"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ただ一つのテストを実行\n",
    "\n",
    "`<module>::<test_name>`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 1 item                                                               \u001b[0m\n\ntest_four.py \u001b[32m.\u001b[0m\u001b[32m                                                           [100%]\u001b[0m\n\n\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_four.py::test_asdict"
   ]
  },
  {
   "source": [
    "## Options"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `--collect-only`: テストケースを集まるだけ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 6 items                                                              \u001b[0m\n\n<Module test_four.py>\n  <Function test_asdict>\n  <Function test_replace>\n<Module test_one.py>\n  <Function test_passing>\n<Module test_three.py>\n  <Function test_defaults>\n  <Function test_member_access>\n<Module test_two.py>\n  <Function test_failing>\n\n\u001b[32m========================== \u001b[32m6 tests collected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --collect-only"
   ]
  },
  {
   "source": [
    "### `-k EXPRESSION`: 式を利用してテストケースを探す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 6 items / 4 deselected / 2 selected                                  \u001b[0m\n",
      "\n",
      "<Module test_four.py>\n",
      "  <Function test_asdict>\n",
      "<Module test_three.py>\n",
      "  <Function test_defaults>\n",
      "\n",
      "\u001b[32m================= \u001b[32m2/6 tests collected (4 deselected)\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -k \"asdict or defaults\" --collect-only"
   ]
  },
  {
   "source": [
    "### `-m MARKEXPR`: 関数をマックして一緒に実行する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.run_these_please  # 任意な名前でいい\n",
    "def test_member_access():\n",
    "    ...\n",
    "```"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 -- /Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 6 items / 4 deselected / 2 selected                                  \u001b[0m\n",
      "\n",
      "test_four.py::test_replace \u001b[32mPASSED\u001b[0m\u001b[32m                                        [ 50%]\u001b[0m\n",
      "test_three.py::test_member_access \u001b[32mPASSED\u001b[0m\u001b[32m                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v -m run_these_please"
   ]
  },
  {
   "source": [
    "### `-x, --exitfirst`: 失敗する場合は継続ではなく停止する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "test_four.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [ 33%]\u001b[0m\n",
      "test_one.py \u001b[32m.\u001b[0m\u001b[32m                                                            [ 50%]\u001b[0m\n",
      "test_three.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [ 83%]\u001b[0m\n",
      "test_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m5 passed\u001b[0m\u001b[31m in 0.28s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 6 items                                                              \u001b[0m\n\ntest_four.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [ 33%]\u001b[0m\ntest_one.py \u001b[32m.\u001b[0m\u001b[32m                                                            [ 50%]\u001b[0m\ntest_three.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [ 83%]\u001b[0m\ntest_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n\n=========================== short test summary info ============================\nFAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m5 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# tb -> traceback\n",
    "!pytest --tb=no"
   ]
  },
  {
   "source": [
    "### `--maxfail=num`: 失敗の数が`num`に達したら停止する\n",
    "\n",
    "`num`が`1`の場合は`-x`と同じ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `--lf, --last-failed`: 最後の失敗を示す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 1 item                                                               \u001b[0m\n",
      "run-last-failure: rerun previous 1 failure (skipped 3 files)\n",
      "\n",
      "test_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.19s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --lf"
   ]
  },
  {
   "source": [
    "### `--ff, --failed-first`: 先に失敗のケースを実行、その後残りのケースを実行する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\ncollected 6 items                                                              \u001b[0m\nrun-last-failure: rerun previous 1 failure first\n\ntest_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [ 16%]\u001b[0m\ntest_four.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                          [ 50%]\u001b[0m\ntest_one.py \u001b[32m.\u001b[0m\u001b[31m                                                            [ 66%]\u001b[0m\ntest_three.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                         [100%]\u001b[0m\n\n=========================== short test summary info ============================\nFAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m5 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --ff --tb=no"
   ]
  },
  {
   "source": [
    "### `-v, --verbose`: 詳細表示"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `-q, --quiet`: `-v, --verbose`の反対"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                   [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Full diff:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?  ^     ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + (1, 2, 3)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?  ^     ^\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m5 passed\u001b[0m\u001b[31m in 0.17s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -q"
   ]
  },
  {
   "source": [
    "### `-l, --showlocals`: トラックバックにローカル変数と値を出力"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# test_four.py\n",
    "t_expected = Task('finish cooking', 'sanji', True, 11)  # 10 -> 11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/maozhongfu/GitHub/自動化/テスト/pytest/01.Getting Start\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "test_four.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                          [ 33%]\u001b[0m\n",
      "test_one.py \u001b[32m.\u001b[0m\u001b[31m                                                            [ 50%]\u001b[0m\n",
      "test_three.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                         [ 83%]\u001b[0m\n",
      "test_two.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_replace _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.run_these_please\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_replace\u001b[39;49;00m():\n",
      "        t_before = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mfinish cooking\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msanji\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m)\n",
      "        t_after = t_before._replace(\u001b[96mid\u001b[39;49;00m=\u001b[94m10\u001b[39;49;00m, done=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        t_expected = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mfinish cooking\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msanji\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m t_after == t_expected\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert Task(summary=...e=True, id=10) == Task(summary=...e=True, id=11)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 3 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing attributes:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ['id']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Drill down into differing attribute id:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           id: 10 != 11...\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         ...Full output truncated (2 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "t_after    = Task(summary='finish cooking', owner='sanji', done=True, id=10)\n",
      "t_before   = Task(summary='finish cooking', owner='sanji', done=False, id=None)\n",
      "t_expected = Task(summary='finish cooking', owner='sanji', done=True, id=11)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_four.py\u001b[0m:29: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________________ test_failing _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == (\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert (1, 2, 3) == (3, 2, 1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 1 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[31mtest_two.py\u001b[0m:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_four.py::test_replace - AssertionError: assert Task(summary=...e=...\n",
      "FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.19s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -l"
   ]
  },
  {
   "source": [
    "### `--tb=style`: 失敗する際のトランスバックを変更する\n",
    "\n",
    "* `no`: 完全に表示しない\n",
    "* `line`: 一行で納める\n",
    "* `short`: `assert`のところだけを出力\n",
    "* `long`: 一番詳しい出力\n",
    "* `auto`: デフォルト値、最初と最後の失敗だけを出力\n",
    "* `native`: pythonの標準ライブラリの出力だけを出力"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `--durations=N`: もっとも遅いの`tests/setups/teardowns`N個を出力\n",
    "\n",
    "> Nが`0`の場合、遅い順で全てを出力"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `--version`: `pytest`をバージョンを出力"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pytest 6.2.1\n"
     ]
    }
   ],
   "source": [
    "!pytest --version"
   ]
  },
  {
   "source": [
    "### `-h, --help`: ヘルプ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}